import streamlit as st
import pandas as pd
from analyzer import segment_text, constants
import thulac
import io
import zipfile
import math
from collections import Counter, defaultdict

# =====================================================
# THULAC â†’ ä¸­æ–‡è¯æ€§æ˜ å°„ï¼ˆå…³é”®ï¼ï¼‰
# =====================================================
# =====================================================
# THULAC â†’ è§„èŒƒä¸­æ–‡è¯æ€§ï¼ˆå”¯ä¸€ã€ç¨³å®šï¼‰
# =====================================================
POS_THULAC_TO_CN = {
    "n": "å",
    "v": "åŠ¨",
    "a": "å½¢",
    "d": "å‰¯",
    "r": "ä»£",
    "q": "é‡",
    "m": "æ•°",
    "p": "ä»‹",
    "u": "åŠ©",      # â­ åªä¿ç•™ä¸€ä¸ª
    "c": "è¿",
    "f": "æ–¹",
    "t": "æ—¶",
    "s": "å¤„",
    "e": "å¹",
    "y": "è¯­",
    "o": "æ‹Ÿ",
    "g": "è¯­ç´ ",
}


# =====================================================
# å·¥å…·å‡½æ•°ï¼šç»Ÿä¸€è¯è¡¨è§„åˆ™ä¸º list
# =====================================================
def ensure_rule_list(info):
    if isinstance(info, list):
        return info
    return [info]


# ===============================
# Streamlit é¡µé¢è®¾ç½®
# ===============================
# st.set_page_config(page_title="æ±‰è¯­è¯æ±‡ç­‰çº§åˆ†æå·¥å…·", layout="wide")
st.set_page_config(
    page_title="æ±‰è¯­è¯æ±‡ç­‰çº§ç»Ÿè®¡å·¥å…·",
    layout="wide",   # å®½å±æ¨¡å¼
    page_icon="ğŸ“Š"
)

# =====================================================
# Session State åˆå§‹åŒ–
# =====================================================
if "df" not in st.session_state:
    st.session_state.df = None

if "excel_bytes" not in st.session_state:
    st.session_state.excel_bytes = None

if "zip_bytes" not in st.session_state:
    st.session_state.zip_bytes = None

# =====================================================
# é¡µé¢æ ‡é¢˜
# =====================================================

# ç”¨ CSS é™åˆ¶ file_uploader çš„åˆ—è¡¨é«˜åº¦
st.markdown(
    """
    <style>
    /* é™åˆ¶ file_uploader çš„é«˜åº¦ä¸º 200pxï¼Œå¤šä½™éƒ¨åˆ†æ»šåŠ¨ */
    div[data-baseweb="file-uploader"] > div:first-child {
        max-height: 200px;
        overflow-y: auto;
    }
    </style>
    """,
    unsafe_allow_html=True
)


# å±…ä¸­æ ‡é¢˜
st.markdown(
    """
    <h1 style="text-align: center;">ğŸ“Š æ±‰è¯­è¯æ±‡ç­‰çº§è‡ªåŠ¨ç»Ÿè®¡å·¥å…·</h1>
    <hr>
    """,
    unsafe_allow_html=True
)
# st.markdown("---")  # åˆ†å‰²çº¿
# ç®€æ´å·¥å…·ä»‹ç»
# ===============================
st.markdown("#### ğŸ¯ å·¥å…·ä»‹ç»")
st.markdown(
    """
- æœ¬å·¥å…·æ”¯æŒæ‰¹é‡ä¸Šä¼  UTF-8 ç¼–ç çš„ TXT æ–‡æœ¬ï¼Œå¯è‡ªåŠ¨ç»Ÿè®¡å„æ–‡æœ¬ä¸­**æ–°HSKï¼ˆ1-9çº§ï¼‰**ã€**æ—§HSKï¼ˆ1-6çº§ï¼‰**åŠ**YCT å°‘å„¿æ±‰è¯­ï¼ˆ1-4çº§ï¼‰** è¯æ±‡ç­‰çº§çš„ **é¢‘æ•°å’Œè¯åºåˆ—**ã€‚<br>
- å·¥å…·è°ƒç”¨ **[THULAC](https://thulac.thunlp.org/)**  (*`THU Lexical Analyzer for Chineseï¼Œæ¸…åå¤§å­¦è‡ªç„¶è¯­è¨€å¤„ç†ä¸ç¤¾ä¼šäººæ–‡è®¡ç®—å®éªŒå®¤ç ”åˆ¶`*)  å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œ
åˆ†è¯åçš„æ–‡æœ¬å¯æ‰“åŒ…ä¸‹è½½ä¸º `segmented_texts.zip`ï¼Œæ–¹ä¾¿å­¦ä¹ ä¸ç ”ç©¶ã€‚
"""
)

# å¹¶æ’æ˜¾ç¤ºåŠŸèƒ½æ¦‚è§ˆå’Œé€‚ç”¨åœºæ™¯
# col1_function, col2_usage = st.columns([3, 1])  # å®½åº¦æ¯”ä¾‹ 3:1

st.markdown("#### âš¡ åŠŸèƒ½æ¦‚è§ˆ")
st.markdown(
    """
1. <b>æ‰¹é‡æ–‡æœ¬ä¸Šä¼ ï¼š</b> æ”¯æŒåŒæ—¶ä¸Šä¼ å¤šä¸ª UTF-8 ç¼–ç  TXT æ–‡ä»¶ã€‚<br>
2. <b>è‡ªåŠ¨ä¸­æ–‡åˆ†è¯ï¼š</b> è°ƒç”¨ <a href="https://thulac.thunlp.org/" target="_blank">**THULAC**</a> è¿›è¡Œé«˜æ•ˆåˆ†è¯ã€‚<br>
3. <b>è¯æ±‡ç­‰çº§ç»Ÿè®¡ï¼š</b> æ ¹æ®æ‰€é€‰è¯è¡¨ï¼ˆæ–°HSKã€æ—§HSKã€YCTå°‘å„¿æ±‰è¯­ï¼‰è‡ªåŠ¨ç»Ÿè®¡å¯¹åº”ç­‰çº§çš„è¯é¢‘å’Œè¯åºåˆ—ã€‚<br>
4. <b>ä¸‹è½½ç»“æœï¼š</b><br>
&nbsp;&nbsp;&nbsp;&nbsp;- <code>è¯é¢‘ç»Ÿè®¡ï¼ˆExcelï¼‰</code>ï¼šå¯¼å‡ºåŒ…å«å„ç­‰çº§è¯é¢‘å’Œè¯åºåˆ—çš„ Excel æ–‡ä»¶ã€‚<br>
&nbsp;&nbsp;&nbsp;&nbsp;- <code>åˆ†è¯æ–‡æœ¬ï¼ˆZIPï¼‰</code>ï¼šæ‰¹é‡ä¸‹è½½åˆ†è¯åçš„ TXT æ–‡æœ¬ï¼Œä¾¿äºä¿å­˜æˆ–äºŒæ¬¡åˆ†æã€‚
""",
    unsafe_allow_html=True
)


st.markdown("---")  # åˆ†å‰²çº¿


# ===============================
# ç”¨æˆ·ä¸Šä¼ æ–‡æœ¬
# ===============================
st.markdown("#### ğŸ“‚ ä¸Šä¼ æ–‡ä»¶")
uploaded_files = st.file_uploader(
    "ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª TXT æ–‡ä»¶", type=["txt"], accept_multiple_files=True
)
st.markdown("---")  # åˆ†å‰²çº¿
st.markdown("### ğŸ—‚ï¸ é€‰æ‹©è¯è¡¨å¹¶ç»Ÿè®¡è¯é¢‘")
# ===============================
col1, col2, col3 = st.columns([2, 0.2, 1])
# ===============================
# ç”¨æˆ·é€‰æ‹©è¯è¡¨å’Œç­‰çº§
# ===============================
with col1:
    wordmap_options = {
        "æ–°HSKç­‰çº§è¯æ±‡ï¼ˆ1-9çº§ï¼‰": constants.NEW_HSK_MAP,
        "æ—§HSKç­‰çº§è¯æ±‡ï¼ˆ1-6çº§ï¼‰": constants.OLD_HSK_MAP,
        "YCTå°‘å„¿æ±‰è¯­ï¼ˆ1-4çº§ï¼‰": constants.YCT_MAP
    }

    selected_wordmap_name = st.selectbox("é€‰æ‹©è¯è¡¨", list(wordmap_options.keys()))
    selected_wordmap = wordmap_options[selected_wordmap_name]
    # è‡ªåŠ¨è·å–è¯¥è¯è¡¨ä¸­æ‰€æœ‰ç­‰çº§
    # selected_levels = sorted({v["level"] for v in selected_wordmap.values()})
    # selected_levels = sorted({v["level"] for v in selected_wordmap.values() if "level" in v})
    selected_levels = sorted(
        {rule["level"] for info in selected_wordmap.values()
         for rule in ensure_rule_list(info)}
    )




    # ===============================
    # ç»Ÿè®¡æŒ‰é’®
    # ===============================
with col3:
    st.markdown(
    """
    <style>
    div.stButton > button:first-child {
        margin-top: 10px;  /* å‘ä¸‹ç§»åŠ¨ 15px */
    }
    </style>
    """,
    unsafe_allow_html=True
)
    do_analysis = st.button("ğŸ“Š ç»Ÿè®¡è¯é¢‘")

# -------------------------------
# ç‚¹å‡»æŒ‰é’®åæ‰§è¡Œç»Ÿè®¡
# -------------------------------
if do_analysis:
    if not uploaded_files:
        st.warning("è¯·å…ˆä¸Šä¼ è‡³å°‘ä¸€ä¸ª TXT æ–‡ä»¶")
    else:
        # ===============================
        # åˆå§‹åŒ– THULAC
        # ===============================
        thu = thulac.thulac(
            seg_only=False,
            user_dict="user_dict/user_dict.txt"
        )

        # è¯»å– POS ä¿®æ­£è¡¨
        pos_fix = constants.load_pos_fix("user_dict/pos_fix.txt")

        # ===============================
        # åˆ†æä¸Šä¼ æ–‡ä»¶
        # ===============================
        results = []
        segmented_files = {}  # å­˜å‚¨æ¯ä¸ªæ–‡ä»¶çš„åˆ†è¯æ–‡æœ¬

        if uploaded_files:
            for uploaded_file in uploaded_files:
                text = uploaded_file.read().decode("utf-8")

                # -------- åˆ†è¯ + POS ä¿®æ­£ --------
                raw_tokens = segment_text(text, thu, pos_fix)
                # -------- æ¸…ç† tokensï¼ˆå»æ‰ç©ºè¯å’Œç©ºæ ‡æ³¨å’Œæ ‡ç‚¹ç¬¦å·ï¼‰ --------
                base_tokens = [(w.replace("\ufeff", "").strip(), p) for w, p in raw_tokens
                               if w and w.replace("\ufeff", "").strip() != "" and p != "w"]

                
                # -------- ä¿å­˜åˆ†è¯æ–‡æœ¬ä¾›ä¸‹è½½ --------
                segmented_text = " ".join([f"{w}/{p}" for w, p in raw_tokens if w.strip() != "" and p.strip() != ""])
                segmented_files[uploaded_file.name] = segmented_text
                
                

                # -------- ç»Ÿè®¡è¯é¢‘å’Œåºåˆ— --------
                level_count = Counter()
                level_words = defaultdict(list)

                used_indices = set()  # â­ å…³é”®ï¼šè®°å½•å“ªäº› token å·²è¢«â€œåƒæ‰â€

                # ===============================
                # æ ¸å¿ƒï¼šæŒ‰è¯ + å¤šè§„åˆ™ + è¯æ€§åŒ¹é…
                # ===============================
                for i, (w, p) in enumerate(base_tokens):
                    if w not in selected_wordmap:
                        continue

                    rules = ensure_rule_list(selected_wordmap[w])
                    p0 = p[0]   # â­ åªå– THULAC è¯æ€§é¦–å­—æ¯# è®¾è®¡é€‰æ‹©ï¼šå°† THULAC å¤åˆè¯æ€§å‹ç¼©ä¸ºé¦–å­—æ¯
                    pos_cn = POS_THULAC_TO_CN.get(p0)

                    for rule in rules:
                        level = rule.get("level")
                        if level not in selected_levels:
                            continue

                        # æœ‰ pos_map æ‰åˆ¤æ–­è¯æ€§
                        if "pos_map" in rule:
                            if pos_cn is None or pos_cn not in rule["pos_map"]:
                                continue

                        # å‘½ä¸­è§„åˆ™
                        level_count[level] += 1
                        level_words[level].append(w)
                        used_indices.add(i)
                        break   # â­ å…³é”®ï¼šåªå‘½ä¸­ä¸€æ¡è§„åˆ™

                # -------- ç»Ÿè®¡éè¯è¡¨è¯æ±‡ï¼ˆå»æ‰æ ‡ç‚¹ï¼‰ --------
                other_words = [
                    base_tokens[i][0]
                    for i in range(len(base_tokens))
                    if i not in used_indices
                ]

                other_freq = len(other_words)
                other_seq = ", ".join(other_words)

                # -------- å»æ‰æ ‡ç‚¹ç»Ÿè®¡æ€»è¯æ•°å’Œä¸åŒè¯æ•° --------
                total_tokens = len(base_tokens)
                total_types = len({w for w, _ in base_tokens})

                type_token_ratio = total_types/math.sqrt(total_tokens) if total_tokens > 0 else 0

                if sum(level_count.values()) + other_freq != total_tokens:
                    st.error(
                        f"ç»Ÿè®¡ä¸ä¸€è‡´ï¼š{sum(level_count.values())} + {other_freq} != {total_tokens}"
                    )
                    st.stop()

                                # -------- ç”Ÿæˆ DataFrame è¡Œ --------
                row = {
                    "æ–‡ä»¶å": uploaded_file.name
                }

                for lv in selected_levels:
                    row[f"{lv}_é¢‘æ•°"] = level_count.get(lv, 0)
                    row[f"{lv}_è¯åºåˆ—"] = ", ".join(level_words.get(lv, []))

                # æ·»åŠ éè¯è¡¨ç»Ÿè®¡
                row["ä¸å±äºå½“å‰è¯è¡¨çš„è¯æ±‡_é¢‘æ•°"] = other_freq
                row["ä¸å±äºå½“å‰è¯è¡¨çš„è¯æ±‡_è¯åºåˆ—"] = other_seq
                row["æ–‡æœ¬_æ€»è¯æ•°_Token"] = total_tokens
                row["æ–‡æœ¬_ä¸åŒè¯æ•°_Type"] = total_types
                row["è¯æ±‡å¤šæ ·æ€§"] = type_token_ratio
                results.append(row)

            
            # -------- æ˜¾ç¤ºç»Ÿè®¡è¡¨ --------
            df = pd.DataFrame(results)
            st.session_state.df = df # ä¿å­˜åˆ° session_state



            # Excel
            # df æ˜¯è¦å¯¼å‡ºçš„ DataFrame
            excel_buf = io.BytesIO()  # å†…å­˜æ–‡ä»¶
            with pd.ExcelWriter(excel_buf, engine='xlsxwriter') as writer:
                df.to_excel(writer, index=False, sheet_name='Sheet1')
                # ä¸éœ€è¦ writer.save()ï¼Œwith ä¼šè‡ªåŠ¨ä¿å­˜
            # é‡ç½®æŒ‡é’ˆ
            excel_buf.seek(0)
            st.session_state.excel_bytes = excel_buf.getvalue()
            
            # -------- æ‰“åŒ…åˆ†è¯æ–‡æœ¬ä¸º zip ä¸‹è½½ --------
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                for fname, content in segmented_files.items():
                    seg_name = fname.replace(".txt", "_seg.txt")
                    zf.writestr(seg_name, content)
            zip_buffer.seek(0)
            st.session_state.zip_bytes = zip_buffer.getvalue()

# =====================================================
# â­ æ°¸ä¹…ç»“æœåŒºï¼ˆå’ŒæŒ‰é’®æ— å…³ï¼‰
# =====================================================
if st.session_state.df is not None:
    st.markdown("#### ğŸ“ˆ ç»Ÿè®¡ç»“æœ")
    st.dataframe(
        st.session_state.df,
        height=200,
        use_container_width=True
    )




    st.markdown("#### ğŸ’¾ ä¸‹è½½ç»“æœ")
    with st.expander("ä¸‹è½½è¯é¢‘ç»Ÿè®¡ï¼ˆExcelï¼‰ã€åˆ†è¯æ–‡æœ¬ï¼ˆZIPï¼‰"):
        download_col1, download_col2 = st.columns([1, 1])
        download_col1.download_button(label="â¬‡ï¸ è¯é¢‘ç»Ÿè®¡ï¼ˆExcelï¼‰",
        data = st.session_state.excel_bytes,
        file_name="è¯æ±‡ç­‰çº§ç»Ÿè®¡ç»“æœ.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            
        download_col2.download_button(label="â¬‡ï¸ åˆ†è¯æ–‡æœ¬ï¼ˆZIPï¼‰",
        data = st.session_state.zip_bytes,
        file_name="segmented_texts.zip",
        mime="application/zip")


## æ·»åŠ footer
st.markdown(
    """
    <style>
    .footer {
        position: fixed;
        left: 0;
        bottom: 0;
        width: 100%;
        background-color: #f8f9fa;
        color: #555;
        text-align: center;
        padding: 5px 0;
        font-size: 0.85rem;
        border-top: 1px solid #ddd;
        z-index: 50;
    }

    /* ç»™é¡µé¢åº•éƒ¨ç•™ç©ºé—´ï¼Œé¿å…å†…å®¹è¢« footer æŒ¡ä½ */
    .block-container {
        padding-bottom: 500px;
    }
    </style>

    <div class="footer">
    <br>Created by: Qin Xu (Kyoto University), Yu zhu (Xiamen University)
        <br>&copy; æ±‰è¯­è¯­è¯æ±‡ç­‰çº§è‡ªåŠ¨ç»Ÿè®¡å·¥å…·. All rights reserved.
        <br>Version 1.0 Â· First released on 2026-01-20</p>

        
    </div>
    """,
    unsafe_allow_html=True
)
